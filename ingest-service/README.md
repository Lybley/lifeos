# LifeOS Ingest Service

A production-ready document ingestion and processing service for the LifeOS Personal Memory Graph. Handles file uploads, OCR, text chunking, embedding generation, and multi-database storage.

## Features

- ğŸ“„ **Multi-format Support**: PDF, DOCX, TXT, PNG, JPG
- ğŸ” **OCR Processing**: Tesseract-based OCR for images and scanned PDFs
- âœ‚ï¸ **Smart Chunking**: Deterministic text chunking with configurable overlap
- ğŸ§  **Embeddings**: Pluggable embedding providers (OpenAI, local models)
- ğŸ’¾ **Multi-Database**: Stores in PostgreSQL, Neo4j, and Pinecone
- ğŸ§ª **Well-Tested**: Comprehensive unit tests with Jest

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Upload    â”‚
â”‚   (File/    â”‚
â”‚    Text)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document      â”‚
â”‚   Processor     â”‚
â”‚  (PDF/DOCX/OCR) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Chunker      â”‚
â”‚  (512 tokens    â”‚
â”‚   + overlap)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embeddings    â”‚
â”‚  (OpenAI/Local) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Multi-DB Storage      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ PostgreSQL         â”‚  â”‚
â”‚ â”‚ - Metadata         â”‚  â”‚
â”‚ â”‚ - Chunks           â”‚  â”‚
â”‚ â”‚ - Vector refs      â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Pinecone           â”‚  â”‚
â”‚ â”‚ - Vector embeddingsâ”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Neo4j              â”‚  â”‚
â”‚ â”‚ - Document nodes   â”‚  â”‚
â”‚ â”‚ - Chunk nodes      â”‚  â”‚
â”‚ â”‚ - Relationships    â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Start

### Prerequisites

- Node.js 18+
- Docker (for Tesseract OCR)
- PostgreSQL, Neo4j, Redis (or use Docker Compose)
- Pinecone account (optional, for vector storage)
- OpenAI API key (or use local embeddings)

### Installation

```bash
# Install dependencies
npm install

# Copy environment template
cp .env.example .env

# Edit .env with your credentials
nano .env
```

### Environment Variables

```bash
# Server
PORT=8001
NODE_ENV=development

# Databases
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=lifeos
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

PINECONE_API_KEY=your_key_here
PINECONE_INDEX=lifeos-vectors

# Embeddings
EMBEDDING_PROVIDER=openai  # or 'local' or 'mock'
EMBEDDING_MODEL=text-embedding-3-small
OPENAI_API_KEY=your_openai_key

# Upload
UPLOAD_DIR=./uploads
```

### Running Locally

```bash
# Development with hot reload
npm run dev

# Build TypeScript
npm run build

# Production
npm start
```

### Running with Docker

```bash
# Build image
docker build -t lifeos-ingest-service .

# Run container
docker run -p 8001:8001 \
  --env-file .env \
  lifeos-ingest-service
```

---

## API Endpoints

### POST /ingest/upload

Upload and process a document file.

**Request:**
```bash
curl -X POST http://localhost:8001/ingest/upload \
  -F "file=@document.pdf" \
  -F "title=Q1 Strategy Document" \
  -F "userId=user-001" \
  -F "maxTokens=512" \
  -F "overlapTokens=50"
```

**Response:**
```json
{
  "success": true,
  "data": {
    "documentId": "doc-a1b2c3d4-...",
    "documentTitle": "Q1 Strategy Document",
    "chunks": 5,
    "embeddings": 5,
    "processingTime": 3450,
    "metadata": {
      "fileName": "document.pdf",
      "fileType": "pdf",
      "fileSize": 245760,
      "pageCount": 10,
      "wordCount": 2450,
      "characterCount": 15600,
      "processingMethod": "direct",
      "embeddingModel": "text-embedding-3-small",
      "embeddingDimensions": 1536,
      "storageResult": {
        "documentId": "doc-a1b2c3d4-...",
        "chunksStored": 5,
        "vectorsStored": 5,
        "neo4jNodesCreated": 6
      }
    }
  }
}
```

**Supported File Types:**
- `application/pdf` - PDF documents
- `application/vnd.openxmlformats-officedocument.wordprocessingml.document` - DOCX
- `text/plain` - Plain text
- `image/png`, `image/jpeg` - Images (with OCR)

---

### POST /ingest/text

Process text content directly without file upload.

**Request:**
```bash
curl -X POST http://localhost:8001/ingest/text \
  -H "Content-Type: application/json" \
  -d '{
    "text": "This is my note about the meeting...",
    "title": "Meeting Notes",
    "userId": "user-001",
    "maxTokens": 512,
    "overlapTokens": 50
  }'
```

**Response:**
```json
{
  "success": true,
  "data": {
    "documentId": "doc-e5f6g7h8-...",
    "documentTitle": "Meeting Notes",
    "chunks": 1,
    "embeddings": 1,
    "processingTime": 850,
    "metadata": {
      "fileType": "text",
      "wordCount": 24,
      "characterCount": 156,
      "processingMethod": "direct"
    }
  }
}
```

---

## Chunking Configuration

The chunker uses a sliding window approach with sentence-level boundaries:

```typescript
{
  maxTokens: 512,      // Maximum tokens per chunk
  overlapTokens: 50,   // Overlap between chunks for context
  minChunkSize: 100    // Minimum chunk size (avoid fragments)
}
```

**How it works:**

1. **Sentence Splitting**: Splits document at sentence boundaries (`.`, `!`, `?`)
2. **Token Estimation**: Estimates tokens using 1 token â‰ˆ 4 characters
3. **Chunk Building**: Adds sentences until token limit
4. **Overlap**: Carries over last N sentences to next chunk
5. **Minimum Size**: Merges tiny final chunks

**Benefits:**
- âœ… Deterministic (same input = same chunks)
- âœ… Preserves sentence boundaries
- âœ… Context continuity via overlap
- âœ… No tiny fragments

---

## Embedding Providers

### OpenAI (Default)

```bash
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-small  # or text-embedding-3-large
EMBEDDING_DIMENSIONS=1536
OPENAI_API_KEY=sk-...
```

**Models:**
- `text-embedding-ada-002`: 1536 dimensions, $0.0001/1K tokens
- `text-embedding-3-small`: 1536 dimensions, $0.00002/1K tokens (recommended)
- `text-embedding-3-large`: 3072 dimensions, $0.00013/1K tokens

### Local Models (Ollama)

```bash
EMBEDDING_PROVIDER=local
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_BASE_URL=http://localhost:11434
EMBEDDING_DIMENSIONS=768
```

**Setup Ollama:**
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull embedding model
ollama pull nomic-embed-text

# Ollama runs on port 11434 by default
```

### Mock (Testing)

```bash
EMBEDDING_PROVIDER=mock
EMBEDDING_MODEL=mock-model
EMBEDDING_DIMENSIONS=1536
```

Generates deterministic vectors for testing without API calls.

---

## Document Processing

### PDF Extraction

- Uses `pdf-parse` for direct text extraction
- Falls back to OCR if text extraction yields < 100 characters
- Preserves page count and structure

### DOCX Extraction

- Uses `mammoth` library
- Extracts raw text with formatting removed
- Handles complex document structures

### Image OCR

- Uses Tesseract.js for OCR processing
- Preprocessing pipeline:
  1. Grayscale conversion
  2. Contrast normalization
  3. Sharpening
- Supports multiple languages (default: English)

**OCR Quality Tips:**
- High-resolution images (300+ DPI)
- Good contrast
- Clean backgrounds
- Horizontal text alignment

---

## Database Storage

### PostgreSQL Tables

**nodes:**
- Document and chunk registry
- Metadata and soft deletes

**vector_embeddings:**
- Pinecone vector references
- Content hashes for deduplication
- Model information

**ingestion_logs:**
- Audit trail
- Processing times
- Error tracking

### Neo4j Graph

**Nodes:**
- `Document`: Main document node
- `Chunk`: Individual chunks

**Relationships:**
- `User -[:OWNS]-> Document`
- `Document -[:HAS_CHUNK]-> Chunk`

### Pinecone Vectors

**Stored data:**
- Vector embeddings (1536 or 3072 dimensions)
- Metadata (node IDs, chunk indices, previews)
- Optimized for similarity search

---

## Testing

### Run Tests

```bash
# All tests
npm test

# Watch mode
npm run test:watch

# Coverage report
npm run test:coverage
```

### Test Coverage

Current coverage:
- Chunker: 95%+
- Embeddings: 90%+
- Overall: 85%+

### Unit Test Examples

**Chunker:**
```typescript
it('should split long text into multiple chunks', () => {
  const longText = 'sentence. '.repeat(100);
  const result = chunkDocument(longText, { maxTokens: 50 });
  
  expect(result.totalChunks).toBeGreaterThan(1);
});
```

**Embeddings:**
```typescript
it('should generate deterministic vectors', async () => {
  const client = new MockEmbeddingClient({ dimensions: 128 });
  const result1 = await client.embed('test');
  const result2 = await client.embed('test');
  
  expect(result1.vector).toEqual(result2.vector);
});
```

---

## Performance

### Benchmarks (MacBook Pro M1)

| File Type | Size | Processing Time | Chunks | Embeddings |
|-----------|------|-----------------|--------|------------|
| PDF (text) | 5 MB | ~2.5s | 50 | 50 |
| PDF (scanned) | 5 MB | ~15s | 30 | 30 |
| DOCX | 1 MB | ~1.2s | 25 | 25 |
| TXT | 500 KB | ~0.8s | 20 | 20 |
| Image OCR | 2 MB | ~8s | 5 | 5 |

**Optimization Tips:**
1. Batch embeddings (100 texts per API call)
2. Use local models for high volume
3. Cache embeddings by content hash
4. Process large files asynchronously

---

## Troubleshooting

### OCR Not Working

**Issue**: Tesseract not found
```bash
Error: tesseract is not installed or it's not in your PATH
```

**Solution**:
```bash
# Ubuntu/Debian
sudo apt-get install tesseract-ocr

# macOS
brew install tesseract

# Docker (already included in Dockerfile)
```

### PostgreSQL Connection Failed

**Issue**: Connection refused
```bash
Error: connect ECONNREFUSED 127.0.0.1:5432
```

**Solution**:
1. Check PostgreSQL is running: `sudo systemctl status postgresql`
2. Verify credentials in `.env`
3. Check network/firewall settings

### Pinecone Vectors Not Stored

**Issue**: Pinecone API key invalid
```bash
Warning: PINECONE_API_KEY not provided, vector features will be disabled
```

**Solution**:
1. Get API key from [Pinecone console](https://app.pinecone.io/)
2. Create index: `lifeos-vectors`
3. Set dimensions: 1536 (for OpenAI ada-002/3-small)
4. Add key to `.env`

### Embedding Generation Slow

**Issue**: OpenAI API rate limits

**Solutions**:
1. Use `text-embedding-3-small` (5x cheaper)
2. Switch to local models (Ollama)
3. Use mock embeddings for testing
4. Implement caching layer

---

## Production Deployment

### Docker Compose

```yaml
version: '3.8'

services:
  ingest-service:
    build: .
    ports:
      - "8001:8001"
    environment:
      - NODE_ENV=production
      - POSTGRES_HOST=postgres
      - NEO4J_URI=bolt://neo4j:7687
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - postgres
      - neo4j
      - redis
```

### Kubernetes

See `/k8s` directory for deployment manifests.

**Key configs:**
- Resource limits: 512Mi memory, 500m CPU
- Health checks: `/health` endpoint
- Volume mounts: `/app/uploads` for temporary files
- Secrets: Database credentials, API keys

### Monitoring

**Metrics to track:**
- Processing time per document
- Chunk count distribution
- Embedding API costs
- Database query performance
- Error rates by file type

**Recommended tools:**
- Prometheus + Grafana
- Sentry for error tracking
- CloudWatch/DataDog for logs

---

## Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/my-feature`
3. Write tests for new functionality
4. Ensure tests pass: `npm test`
5. Lint code: `npm run lint`
6. Submit pull request

---

## License

MIT

---

## Support

- ğŸ“– [Documentation](../docs/)
- ğŸ› [Issue Tracker](https://github.com/lifeos/issues)
- ğŸ’¬ [Discussions](https://github.com/lifeos/discussions)
